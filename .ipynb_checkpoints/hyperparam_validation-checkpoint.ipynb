{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b5d850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044b0279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Setup ---\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f524bb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded and preprocessed with shape: torch.Size([886, 4])\n"
     ]
    }
   ],
   "source": [
    "swiss_data = pd.read_csv('D1_Swiss_processed.csv')\n",
    "FEATURE_COLUMNS = ['Depression', 'Anxiety', 'Burnout', 'Stress']\n",
    "feature_tensor = torch.tensor(swiss_data[FEATURE_COLUMNS].values, dtype=torch.float32)\n",
    "training_dataset = TensorDataset(feature_tensor)\n",
    "train_dataloader = DataLoader(training_dataset, batch_size=64, shuffle=True)\n",
    "print(\"Data loaded and preprocessed with shape:\", feature_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748c7458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Autoencoder Architecture ---\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, hidden_layer_size, latent_dimension=2, activation_function='ReLU'):\n",
    "        super().__init__()\n",
    "        activation_dict = {'ReLU': nn.ReLU, 'Sigmoid': nn.Sigmoid, 'Tanh': nn.Tanh}\n",
    "        activation_layer = activation_dict[activation_function]\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(4, hidden_layer_size),\n",
    "            activation_layer(),\n",
    "            nn.Linear(hidden_layer_size, latent_dimension),\n",
    "            activation_layer()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dimension, hidden_layer_size),\n",
    "            activation_layer(),\n",
    "            nn.Linear(hidden_layer_size, 4),\n",
    "            nn.Identity()\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        encoded_representation = self.encoder(input_data)\n",
    "        reconstructed_output = self.decoder(encoded_representation)\n",
    "        return reconstructed_output, encoded_representation\n",
    "\n",
    "def train_single_configuration(model, optimizer, loss_function, dataloader, num_epochs=20):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    \n",
    "    for current_epoch in range(num_epochs):\n",
    "        total_batch_loss = 0\n",
    "        for batch_data in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            reconstruction, _ = model(batch_data[0])\n",
    "            batch_loss = loss_function(reconstruction, batch_data[0])\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            total_batch_loss += batch_loss.item()\n",
    "        \n",
    "        average_epoch_loss = total_batch_loss / len(dataloader)\n",
    "        epoch_losses.append(average_epoch_loss)\n",
    "    \n",
    "    return epoch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dffb5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Hyperparameter Grid Search ---\n",
    "hidden_layer_sizes = [3, 4, 5, 6, 8, 10]\n",
    "latent_dimensions = [2, 3]  # test interpretability vs reconstruction trade-off\n",
    "optimizer_configs = {\n",
    "    'Adam': lambda model_params: optim.Adam(model_params, lr=1e-3, betas=(0.9, 0.999)),\n",
    "    'SGD':  lambda model_params: optim.SGD(model_params, lr=1e-3, momentum=0.9)\n",
    "}\n",
    "training_epochs = [20, 50, 100]\n",
    "random_seeds = [0, 42, 123]\n",
    "\n",
    "loss_criterion = nn.MSELoss()\n",
    "experiment_results = defaultdict(list)\n",
    "\n",
    "total_experiments = len(hidden_layer_sizes) * len(latent_dimensions) * len(optimizer_configs) * len(training_epochs) * len(random_seeds)\n",
    "current_experiment = 0\n",
    "\n",
    "for hidden_size in hidden_layer_sizes:\n",
    "    for latent_dim in latent_dimensions:\n",
    "        for optimizer_name, optimizer_factory in optimizer_configs.items():\n",
    "            for epochs in training_epochs:\n",
    "                for seed in random_seeds:\n",
    "                    current_experiment += 1\n",
    "                    print(f\"Running experiment {current_experiment}/{total_experiments}: \"\n",
    "                          f\"hidden={hidden_size}, latent={latent_dim}, opt={optimizer_name}, \"\n",
    "                          f\"epochs={epochs}, seed={seed}\")\n",
    "                    \n",
    "                    torch.manual_seed(seed)\n",
    "                    np.random.seed(seed)\n",
    "                    \n",
    "                    autoencoder_model = Autoencoder(\n",
    "                        hidden_layer_size=hidden_size, \n",
    "                        latent_dimension=latent_dim, \n",
    "                        activation_function='ReLU'\n",
    "                    )\n",
    "                    model_optimizer = optimizer_factory(autoencoder_model.parameters())\n",
    "                    \n",
    "                    experiment_start_time = time.time()\n",
    "                    training_losses = train_single_configuration(\n",
    "                        autoencoder_model, model_optimizer, loss_criterion, \n",
    "                        train_dataloader, num_epochs=epochs\n",
    "                    )\n",
    "                    experiment_duration = time.time() - experiment_start_time\n",
    "                    \n",
    "                    config_key = (hidden_size, latent_dim, optimizer_name, epochs)\n",
    "                    experiment_results[config_key].append({\n",
    "                        'random_seed': seed,\n",
    "                        'final_training_loss': training_losses[-1],\n",
    "                        'minimum_training_loss': min(training_losses),\n",
    "                        'training_time_seconds': experiment_duration\n",
    "                    })\n",
    "\n",
    "print(\"\\nHyperparameter validation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6304ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Results Aggregation and Analysis ---\n",
    "aggregated_results = []\n",
    "\n",
    "for configuration_key, experimental_runs in experiment_results.items():\n",
    "    hidden_size, latent_dim, optimizer_name, epochs = configuration_key\n",
    "    \n",
    "    final_losses = [run['final_training_loss'] for run in experimental_runs]\n",
    "    minimum_losses = [run['minimum_training_loss'] for run in experimental_runs]\n",
    "    training_times = [run['training_time_seconds'] for run in experimental_runs]\n",
    "    \n",
    "    aggregated_results.append({\n",
    "        'hidden_layer_size': hidden_size,\n",
    "        'latent_dimension': latent_dim,\n",
    "        'optimizer_type': optimizer_name,\n",
    "        'training_epochs': epochs,\n",
    "        'mean_final_loss': np.mean(final_losses),\n",
    "        'std_final_loss': np.std(final_losses),\n",
    "        'mean_minimum_loss': np.mean(minimum_losses),\n",
    "        'std_minimum_loss': np.std(minimum_losses),\n",
    "        'mean_training_time': np.mean(training_times),\n",
    "        'std_training_time': np.std(training_times)\n",
    "    })\n",
    "\n",
    "results_dataframe = pd.DataFrame(aggregated_results)\n",
    "results_dataframe = results_dataframe.sort_values([\n",
    "    'hidden_layer_size', 'latent_dimension', 'optimizer_type', 'training_epochs'\n",
    "])\n",
    "\n",
    "print(\"=== HYPERPARAMETER VALIDATION RESULTS ===\")\n",
    "print(results_dataframe.to_string(index=False))\n",
    "\n",
    "# Find best performing configuration\n",
    "best_config_idx = results_dataframe['mean_final_loss'].idxmin()\n",
    "best_configuration = results_dataframe.loc[best_config_idx]\n",
    "\n",
    "print(f\"\\n=== BEST CONFIGURATION ===\")\n",
    "print(f\"Hidden Layer Size: {best_configuration['hidden_layer_size']}\")\n",
    "print(f\"Latent Dimension: {best_configuration['latent_dimension']}\")\n",
    "print(f\"Optimizer: {best_configuration['optimizer_type']}\")\n",
    "print(f\"Training Epochs: {best_configuration['training_epochs']}\")\n",
    "print(f\"Mean Final Loss: {best_configuration['mean_final_loss']:.6f}\")\n",
    "print(f\"Mean Training Time: {best_configuration['mean_training_time']:.2f}s\")\n",
    "\n",
    "# --- SAVE RESULTS FOR LOCAL ANALYSIS ---\n",
    "# Save all results as CSV\n",
    "results_dataframe.to_csv('hyperparameter_results.csv', index=False)\n",
    "print(\"\\n Results saved to 'hyperparameter_results.csv'\")\n",
    "\n",
    "# Save best config as JSON\n",
    "import json\n",
    "best_config_dict = {\n",
    "    'hidden_layer_size': int(best_configuration['hidden_layer_size']),\n",
    "    'latent_dimension': int(best_configuration['latent_dimension']),\n",
    "    'optimizer_type': best_configuration['optimizer_type'],\n",
    "    'training_epochs': int(best_configuration['training_epochs']),\n",
    "    'mean_final_loss': float(best_configuration['mean_final_loss']),\n",
    "    'mean_training_time': float(best_configuration['mean_training_time'])\n",
    "}\n",
    "\n",
    "with open('best_hyperparameters.json', 'w') as f:\n",
    "    json.dump(best_config_dict, f, indent=2)\n",
    "print(\" Best configuration saved to 'best_hyperparameters.json'\")\n",
    "\n",
    "# Save raw experiment data\n",
    "import pickle\n",
    "with open('experiment_results.pkl', 'wb') as f:\n",
    "    pickle.dump(dict(experiment_results), f)\n",
    "print(\" Raw experiment data saved to 'experiment_results.pkl'\")\n",
    "\n",
    "# Create summary report\n",
    "with open('validation_summary.txt', 'w') as f:\n",
    "    f.write(\"=== HYPERPARAMETER VALIDATION SUMMARY ===\\n\")\n",
    "    f.write(f\"Total experiments run: {total_experiments}\\n\")\n",
    "    f.write(f\"Best configuration:\\n\")\n",
    "    f.write(f\"  Hidden Layer Size: {best_configuration['hidden_layer_size']}\\n\")\n",
    "    f.write(f\"  Latent Dimension: {best_configuration['latent_dimension']}\\n\")\n",
    "    f.write(f\"  Optimizer: {best_configuration['optimizer_type']}\\n\")\n",
    "    f.write(f\"  Training Epochs: {best_configuration['training_epochs']}\\n\")\n",
    "    f.write(f\"  Mean Final Loss: {best_configuration['mean_final_loss']:.6f}\\n\")\n",
    "    f.write(f\"  Mean Training Time: {best_configuration['mean_training_time']:.2f}s\\n\")\n",
    "\n",
    "print(\" Summary report saved to 'validation_summary.txt'\")\n",
    "print(\"\\n All results saved! Ready to transfer back to local machine.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9e0945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Visualization of Results ---\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Configuration for comparison plot\n",
    "comparison_hidden_size = 5\n",
    "comparison_epochs = 50\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "for latent_dim in latent_dimensions:\n",
    "    for optimizer_name in optimizer_configs.keys():\n",
    "        filter_mask = (results_dataframe['hidden_layer_size'] == comparison_hidden_size) & \\\n",
    "                     (results_dataframe['latent_dimension'] == latent_dim) & \\\n",
    "                     (results_dataframe['optimizer_type'] == optimizer_name) & \\\n",
    "                     (results_dataframe['training_epochs'] == comparison_epochs)\n",
    "        \n",
    "        if filter_mask.any():\n",
    "            loss_value = results_dataframe[filter_mask]['mean_final_loss'].values[0]\n",
    "            plt.bar(f\"{optimizer_name}_{latent_dim}D\", loss_value)\n",
    "\n",
    "plt.ylabel(\"Mean Final Loss\")\n",
    "plt.title(f\"Optimizer & Latent Dimension Comparison\\nHidden={comparison_hidden_size}, Epochs={comparison_epochs}\")\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Loss by hidden layer size\n",
    "plt.subplot(2, 2, 2)\n",
    "hidden_size_losses = results_dataframe.groupby('hidden_layer_size')['mean_final_loss'].mean()\n",
    "plt.plot(hidden_size_losses.index, hidden_size_losses.values, marker='o')\n",
    "plt.xlabel(\"Hidden Layer Size\")\n",
    "plt.ylabel(\"Mean Final Loss\")\n",
    "plt.title(\"Performance vs Hidden Layer Size\")\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Loss by training epochs\n",
    "plt.subplot(2, 2, 3)\n",
    "epoch_losses = results_dataframe.groupby('training_epochs')['mean_final_loss'].mean()\n",
    "plt.plot(epoch_losses.index, epoch_losses.values, marker='s')\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Mean Final Loss\")\n",
    "plt.title(\"Performance vs Training Epochs\")\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Training time vs performance\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(results_dataframe['mean_training_time'], results_dataframe['mean_final_loss'], \n",
    "           alpha=0.6, c=results_dataframe['hidden_layer_size'], cmap='viridis')\n",
    "plt.xlabel(\"Mean Training Time (seconds)\")\n",
    "plt.ylabel(\"Mean Final Loss\")\n",
    "plt.title(\"Training Time vs Performance\")\n",
    "plt.colorbar(label='Hidden Layer Size')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
